{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4222b89e",
   "metadata": {},
   "source": [
    "$\\newcommand{\\Hset}{\\mathcal{H}}$\n",
    "$\\newcommand{\\Aset}{\\mathcal{A}}$\n",
    "$\\newcommand{\\Sset}{\\mathcal{S}}$\n",
    "$\\newcommand{\\Tset}{\\mathcal{T}}$\n",
    "$\\newcommand{\\HAinter}{\\Hset \\cap \\Aset}$\n",
    "$\\newcommand{\\HAunion}{\\Hset \\cup \\Aset}$\n",
    "$\\newcommand{\\Hsize}{\\left|\\Hset\\right|}$\n",
    "$\\newcommand{\\Asize}{\\left|\\Aset\\right|}$\n",
    "$\\newcommand{\\Tsize}{\\left|\\Tset\\right|}$\n",
    "$\\newcommand{\\HAunionsize}{\\left|\\HAunion\\right|}$\n",
    "$\\newcommand{\\HAintersize}{\\left|\\HAinter\\right|}$\n",
    "$\\newcommand{\\lo}{\\mathrm{lo}}$\n",
    "$\\newcommand{\\hi}{\\mathrm{hi}}$\n",
    "$\\newcommand{\\expL}{\\mathcal{L}}$\n",
    "$\\newcommand{\\expLopt}{\\mathcal{L}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\logit}{\\mathrm{logit}}$\n",
    "$\\newcommand{\\expit}{\\mathrm{expit}}$\n",
    "$\\newcommand{\\Var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\Cov}{\\mathrm{Cov}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Sampling Model — Interactive Simulation\n",
    "\n",
    "This notebook simulates binary classification with a set of $n$ binary signals,\n",
    "$$\n",
    "\\Sset = \\{ S_1, \\dots, S_n \\}, \\quad S_i \\in \\{\\lo, \\hi\\}.\n",
    "$$\n",
    "\n",
    "A human and an AI each observe subsets of i.i.d. binary signals, denoted $\\Hset$ and $\\Aset$ respectively. \n",
    "Signals in the overlapping set $\\Hset \\cap \\Aset$ are observed by both agents.\n",
    "\n",
    "\n",
    "## Model\n",
    "\n",
    "The model has the following parameters:\n",
    "- $n := |\\Sset| \\in \\mathbb{N}$, the total number of i.i.d. signals\n",
    "- $p_Y := \\Pr (Y=1) \\in (0,1)$, the prior probability of $Y\\in \\{0,1\\}$\n",
    "- $p_0 := \\Pr (S_i = \\hi \\mid Y=0) \\in (0,1)$, the likelihood of each $S_i$ when $Y=0$\n",
    "- $p_1 := \\Pr (S_i = \\hi \\mid Y=1) \\in (0,1)$, the likelihood of each $S_i$ when $Y=1$\n",
    "- $\\Hsize := |\\Hset| \\in \\mathbb{N}$, the number of signals observed by the human\n",
    "- $\\Asize := |\\Aset| \\in \\mathbb{N}$, the number of signals observed by the AI\n",
    "- $\\HAintersize := |\\Hset \\cap \\Aset| \\in \\mathbb{N}$, the number of overlapping signals observed by both agents\n",
    "\n",
    "The model uses log loss\n",
    "$$\n",
    "L(y,d) := -\\big( y \\log d + (1-y) \\log (1-d) \\big),\n",
    "$$\n",
    "so that when $d$ is the true posterior, the expected loss equals the conditional entropy.\n",
    "\n",
    "We assume $p_1 > p_0$, non-redundancy, non-degeneracy, each agent is rational (minimizes expected loss given their model), and most importantly, the human is *dependence-neglect* and believes that $\\Hset \\perp \\Aset \\mid Y$.\n",
    "\n",
    "\n",
    "## Signal transformations\n",
    "\n",
    "For each $\\Tset \\subseteq \\Sset$ (for example, $\\Tset$ may be $\\Hset$, $\\Aset$, or $\\HAunion$):\n",
    "\n",
    "- $$\n",
    "  \\Sigma_\\Tset := \\sum_{S_i \\in \\Tset} \\mathbf{1} [ S_i=\\hi ],\n",
    "  $$\n",
    "  the number of high signals in $\\Tset$.\n",
    "- $$\n",
    "  T := \\Pr ( Y=1 \\mid \\Tset),\n",
    "  $$\n",
    "  the calibrated posterior probability estimate when a (rational, correctly specified) agent observes $\\Tset$.\n",
    "- $$\n",
    "  \\ell_T := \\log \\frac{T}{1-T} = \\log \\frac{\\Pr ( Y=1 \\mid \\Tset)}{\\Pr ( Y=0 \\mid \\Tset)},\n",
    "  $$\n",
    "  the posterior log odds when an agent observes $\\Tset$.\n",
    "\n",
    "Note that all three signal representations, $\\Sigma_\\Tset$, $T$, and $\\ell_T$, are monotonically increasing in each other, and are sufficient statistics for the signals in this model.\n",
    "\n",
    "\n",
    "## Quantities of interest\n",
    "\n",
    "We are interested in:\n",
    "\n",
    "- **Human-only loss**:\n",
    "  $$\n",
    "  \\expLopt (H) \n",
    "  := \\mathbb{E} \\Big[ \\inf_\\delta L(Y, \\delta(H)) \\Big] \n",
    "  = \\mathbb{E} \\big[ L(Y, \\Pr (Y=1\\mid H)) \\big] \n",
    "  = H(Y\\mid H).\n",
    "  $$\n",
    "\n",
    "- **AI-only loss**:\n",
    "  $$\n",
    "  \\expLopt (A) = H(Y\\mid A).\n",
    "  $$\n",
    "\n",
    "- **Optimal joint loss**:\n",
    "  $$\n",
    "  \\expLopt (H,A) = H(Y\\mid H,A).\n",
    "  $$\n",
    "\n",
    "- **Misspecified joint loss** (under dependence neglect):\n",
    "  $$\n",
    "  \\expL(\\widehat{\\delta} (H,A)) \n",
    "  = \\mathbb{E}\\big[ L(Y, \\widehat{\\delta} (H,A)) \\big],\n",
    "  $$\n",
    "  where $\\widehat{\\delta} (H,A)$ is the posterior expectation of $Y$ when assuming $\\Hset \\perp \\Aset \\mid Y$.\n",
    "\n",
    "- **Value of AI signal**:\n",
    "  $$\n",
    "  v(A) := \\expLopt (\\emptyset) - \\expLopt (A) \n",
    "  = H(Y) - H(Y\\mid A) = I(Y;A).\n",
    "  $$\n",
    "\n",
    "- **Marginal value of $A$ given $H$**:\n",
    "  $$\n",
    "  v(A\\mid H) := \\expLopt (H) - \\expLopt (H,A) \n",
    "  = H(Y\\mid H) - H(Y\\mid H,A) = I(Y;A\\mid H).\n",
    "  $$\n",
    "\n",
    "- **Candidates for overlap coefficient**:\n",
    "  * $ \\alpha := \\frac {\\mathrm{Cov}(\\ell_H, \\ell_A \\mid Y)} {\\mathrm{Var}(\\ell_H \\mid Y)}, $ using the same formula as the Gaussian model\n",
    "  * $ \\alpha := \\frac {|\\HAinter|}{|\\Aset|}$, using the same resulting expression as the Gaussian _sampling_ model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23a6e0",
   "metadata": {},
   "source": [
    "## Properties\n",
    "\n",
    "\n",
    "### Posteriors and decision functions\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\logit(x) := \\log \\frac{x}{1-x}, \\quad \\ell_Y := \\logit(p_Y),\n",
    "$$\n",
    "$$\n",
    "\\ell_{\\lo} := \\log \\frac{\\Pr(S_i = \\lo \\mid Y=1)}{\\Pr(S_i = \\lo \\mid Y=0)} \n",
    "= \\log \\frac{1-p_1}{1-p_0},\n",
    "$$\n",
    "$$\n",
    "\\ell_{\\hi} := \\log \\frac{\\Pr(S_i = \\hi \\mid Y=1)}{\\Pr(S_i = \\hi \\mid Y=0)} \n",
    "= \\log \\frac{p_1}{p_0}.\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "\\ell_H \n",
    "= \\ell_Y + \\ell_{\\lo} \\Hsize + (\\ell_{\\hi} - \\ell_{\\lo}) \\Sigma_H,\n",
    "$$\n",
    "$$\n",
    "\\ell_A \n",
    "= \\ell_Y + \\ell_{\\lo} \\Asize + (\\ell_{\\hi} - \\ell_{\\lo}) \\Sigma_A.\n",
    "$$\n",
    "\n",
    "For the optimal joint posterior based on the union $\\HAunion := \\Hset \\cup \\Aset$,\n",
    "$$\n",
    "\\ell_{H, A} \n",
    "= \\ell_Y + \\ell_{\\lo} |\\HAunion| + (\\ell_{\\hi} - \\ell_{\\lo}) \\Sigma_{\\HAunion}.\n",
    "$$\n",
    "\n",
    "The misspecified joint posterior $\\widehat{\\delta}(H,A) = \\widehat{\\Pr}(Y=1\\mid H,A)$ is derived from\n",
    "$$\\begin{align*}\n",
    "\\widehat{\\ell}_{H, A} \n",
    "&= \\ell_H + \\ell_A - \\ell_Y  \\\\\n",
    "&= \\ell_Y + \\ell_{\\lo} (\\Hsize + \\Asize) + (\\ell_{\\hi} - \\ell_{\\lo}) (\\Sigma_H + \\Sigma_A).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In all cases, the posterior probability is obtained via the logistic function:\n",
    "$$\n",
    "\\Pr(Y=1\\mid \\Tset) = \\frac{1}{1 + e^{-\\ell_\\Tset}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### Variances, covariances, and overlap\n",
    "\n",
    "For each $y\\in \\{0,1\\}$, let $p_y := \\Pr(S_i=\\hi \\mid Y=y)$ (so $p_0, p_1$). Then\n",
    "$$\n",
    "\\Var ( \\ell_H \\mid Y=y) \n",
    "= (\\ell_\\hi - \\ell_\\lo)^2 \\,\\Hsize\\, p_y (1-p_y),\n",
    "$$\n",
    "$$\n",
    "\\Cov ( \\ell_H, \\ell_A \\mid Y=y) \n",
    "= (\\ell_\\hi - \\ell_\\lo)^2 \\,\\HAintersize\\, p_y (1-p_y).\n",
    "$$\n",
    "\n",
    "Thus, the “candidate” overlap coefficient is\n",
    "$$\n",
    "\\alpha \n",
    ":= \\frac{\\Cov ( \\ell_H, \\ell_A \\mid Y=y)} {\\Var ( \\ell_H \\mid Y=y)} \n",
    "= \\frac{\\HAintersize}{\\Hsize},\n",
    "\\quad \\forall y\\in \\{0,1\\}.\n",
    "$$\n",
    "\n",
    "Note that $\\alpha$ does not depend on $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d354736",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, FloatSlider\n",
    "    import ipywidgets as widgets\n",
    "    HAS_WIDGETS = True\n",
    "except ImportError:\n",
    "    HAS_WIDGETS = False\n",
    "    print(\"ipywidgets not installed. Run `pip install ipywidgets` and restart the kernel.\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    " \n",
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def logit(p):\n",
    "    p = np.clip(p, 1e-15, 1-1e-15)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def binom_pmf(k, n, p):\n",
    "    \"\"\"Binomial pmf: P(K=k) for K~Bin(n,p).\"\"\"\n",
    "    if k < 0 or k > n:\n",
    "        return 0.0\n",
    "    return math.comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
    "\n",
    "def log_loss(y, t, eps=1e-15):\n",
    "    \"\"\"Binary log loss (negative log-likelihood).\"\"\"\n",
    "    t = np.clip(t, eps, 1-eps)\n",
    "    return - (y * np.log(t) + (1-y) * np.log(1-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddce3cb",
   "metadata": {},
   "source": [
    "### Defunct: Monte Carlo\n",
    "\n",
    "The code blocks below simulate by generating random signals, which we didn't end up using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: simulate one trial\n",
    "def simulate_one_trial(n_H, n_A, n_both, p_Y, p0, p1, rng=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    n_both = min(n_both, n_H, n_A)\n",
    "    n_H_only = max(n_H - n_both, 0)\n",
    "    n_A_only = max(n_A - n_both, 0)\n",
    "\n",
    "    # sample label\n",
    "    y = rng.binomial(1, p_Y)\n",
    "    p_y = p1 if y == 1 else p0\n",
    "\n",
    "    ell_Y = logit(p_Y)\n",
    "    ell_hi = np.log(p1 / p0)\n",
    "    ell_lo = np.log((1 - p1) / (1 - p0))\n",
    "\n",
    "    # sample hi count for each signal group\n",
    "    hi_both   = rng.binomial(n_both,   p_y)\n",
    "    hi_H_only = rng.binomial(n_H_only, p_y)\n",
    "    hi_A_only = rng.binomial(n_A_only, p_y)\n",
    "\n",
    "    # H-only posterior\n",
    "    hi_H = hi_both + hi_H_only\n",
    "    n_H_total = n_both + n_H_only\n",
    "    T_H = compute_posterior_from_counts(hi_H, n_H_total, ell_Y, ell_hi, ell_lo) if n_H_total > 0 else p_Y\n",
    "\n",
    "    # A-only posterior\n",
    "    hi_A = hi_both + hi_A_only\n",
    "    n_A_total = n_both + n_A_only\n",
    "    T_A = compute_posterior_from_counts(hi_A, n_A_total, ell_Y, ell_hi, ell_lo) if n_A_total > 0 else p_Y\n",
    "\n",
    "    # joint posterior\n",
    "    hi_joint = hi_both + hi_H_only + hi_A_only\n",
    "    n_joint = n_both + n_H_only + n_A_only\n",
    "    T_joint = compute_posterior_from_counts(hi_joint, n_joint, ell_Y, ell_hi, ell_lo) if n_joint > 0 else p_Y\n",
    "\n",
    "    return y, p_Y, T_H, T_A, T_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: expected log loss estimator\n",
    "def log_loss(y, t, eps=1e-12):\n",
    "    t = np.clip(t, eps, 1 - eps)\n",
    "    return - (y * np.log(t) + (1 - y) * np.log(1 - t))\n",
    "\n",
    "def estimate_losses(\n",
    "    n_H, n_A, n_both,\n",
    "    p_Y=0.5, p0=0.2, p1=0.8,\n",
    "    n_trials=20000, seed=None\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    L_prior = 0\n",
    "    L_H = 0\n",
    "    L_A = 0\n",
    "    L_joint = 0\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        y, T_prior, T_H, T_A, T_joint = simulate_one_trial(\n",
    "            n_H, n_A, n_both, p_Y, p0, p1, rng=rng\n",
    "        )\n",
    "\n",
    "        L_prior += log_loss(y, T_prior)\n",
    "        L_H     += log_loss(y, T_H)\n",
    "        L_A     += log_loss(y, T_A)\n",
    "        L_joint += log_loss(y, T_joint)\n",
    "\n",
    "    return {\n",
    "        \"prior\": L_prior / n_trials,\n",
    "        \"H\":     L_H     / n_trials,\n",
    "        \"A\":     L_A     / n_trials,\n",
    "        \"joint\": L_joint / n_trials\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: loss plotting\n",
    "def plot_losses(\n",
    "    n_H=10, n_A=10, n_both=5,\n",
    "    p_Y=0.5, p0=0.3, p1=0.7,\n",
    "    n_trials=20000, seed=0\n",
    "):\n",
    "    n_both = min(n_both, n_H, n_A)\n",
    "    results = estimate_losses(n_H, n_A, n_both, p_Y, p0, p1, n_trials, seed)\n",
    "\n",
    "    labels = [\"prior\", \"H\", \"A\", \"joint\"]\n",
    "    values = [results[k] for k in labels]\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    bars = plt.bar(labels, values)\n",
    "    plt.ylabel(\"Expected log loss\")\n",
    "    plt.title(\n",
    "        f\"p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f}, |H|={n_H}, |A|={n_A}, |H∩A|={n_both}\"\n",
    "    )\n",
    "\n",
    "    for b, v in zip(bars, values):\n",
    "        plt.text(b.get_x() + b.get_width()/2, v, f\"{v:.3f}\",\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    plt.ylim(0, max(values) * 1.1)\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb9cf2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061a23e00af64894bb7335f7ae2c047f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='|H|', max=50), IntSlider(value=10, description='|A|', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: interactive sliders (ipywidgets)\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        plot_losses,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=50, step=1, value=10),\n",
    "        n_A=IntSlider(description=\"|A|\", min=0, max=50, step=1, value=10),\n",
    "        n_both=IntSlider(description=\"|H∩A|\", min=0, max=50, step=1, value=5),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\", min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\", min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "        n_trials=IntSlider(description=\"n_trials\", min=1000, max=50000, step=1000, value=10000),\n",
    "        seed=IntSlider(description=\"seed\", min=0, max=9999, step=1, value=0)\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use sliders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2743e",
   "metadata": {},
   "source": [
    "### New: Compute distributional parameters directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "212e9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_losses(\n",
    "    n_H, n_A, n_both,\n",
    "    p_Y=0.5, p0=0.3, p1=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute expected losses exactly (via finite sums over binomial counts):\n",
    "      - prior-only loss H(Y)\n",
    "      - human-only H(Y|H)\n",
    "      - AI-only H(Y|A)\n",
    "      - optimal joint H(Y|H,A)\n",
    "      - misspecified joint loss under dependence neglect\n",
    "    \n",
    "    Returns a dict of losses and some derived values.\n",
    "    \"\"\"\n",
    "    # sanitize overlap\n",
    "    n_both = min(n_both, n_H, n_A)\n",
    "    n_H_only = max(n_H - n_both, 0)\n",
    "    n_A_only = max(n_A - n_both, 0)\n",
    "    \n",
    "    # prior log-odds and per-signal log-likelihood ratios\n",
    "    ell_Y  = logit(p_Y)\n",
    "    ell_hi = np.log(p1 / p0)\n",
    "    ell_lo = np.log((1-p1) / (1-p0))\n",
    "    \n",
    "    # union size\n",
    "    n_union = n_both + n_H_only + n_A_only\n",
    "    n_H_tot = n_both + n_H_only\n",
    "    n_A_tot = n_both + n_A_only\n",
    "    \n",
    "    # For the true model, expected prior loss = H(Y)\n",
    "    prior_loss = - (p_Y * np.log(p_Y) + (1-p_Y) * np.log(1-p_Y))\n",
    "    \n",
    "    # Initialize expectations\n",
    "    loss_H      = 0.0  # H(Y|H) when using true posterior given H\n",
    "    loss_A      = 0.0  # H(Y|A)\n",
    "    loss_joint  = 0.0  # H(Y|H,A) using optimal posterior given union\n",
    "    loss_mis    = 0.0  # expected loss using mis-specified posterior (dependence neglect)\n",
    "    \n",
    "    # Sum over Y in {0,1} and counts in the three groups\n",
    "    for y in (0, 1):\n",
    "        w_y = p_Y if y == 1 else (1 - p_Y)\n",
    "        p_sig = p1 if y == 1 else p0  # P(S=1 | Y=y)\n",
    "        \n",
    "        # Precompute binomial pmfs for each group given Y\n",
    "        pmf_both = [binom_pmf(k, n_both,   p_sig) for k in range(n_both+1)]\n",
    "        pmf_Ho   = [binom_pmf(k, n_H_only, p_sig) for k in range(n_H_only+1)]\n",
    "        pmf_Ao   = [binom_pmf(k, n_A_only, p_sig) for k in range(n_A_only+1)]\n",
    "        \n",
    "        for k_b in range(n_both+1):\n",
    "            p_b = pmf_both[k_b]\n",
    "            for k_Ho in range(n_H_only+1):\n",
    "                p_Ho = pmf_Ho[k_Ho]\n",
    "                for k_Ao in range(n_A_only+1):\n",
    "                    p_Ao = pmf_Ao[k_Ao]\n",
    "                    \n",
    "                    prob = w_y * p_b * p_Ho * p_Ao\n",
    "                    if prob == 0.0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Counts seen by H, A, and union\n",
    "                    sig_H = k_b + k_Ho\n",
    "                    sig_A = k_b + k_Ao\n",
    "                    sig_U = k_b + k_Ho + k_Ao  # union\n",
    "                    \n",
    "                    # log-odds and posteriors\n",
    "                    # H-only\n",
    "                    ell_H = ell_Y + n_H_tot * ell_lo + sig_H * (ell_hi - ell_lo)\n",
    "                    T_H   = logistic(ell_H)\n",
    "                    \n",
    "                    # A-only\n",
    "                    ell_A = ell_Y + n_A_tot * ell_lo + sig_A * (ell_hi - ell_lo)\n",
    "                    T_A   = logistic(ell_A)\n",
    "                    \n",
    "                    # optimal joint: based on union\n",
    "                    ell_U = ell_Y + n_union * ell_lo + sig_U * (ell_hi - ell_lo)\n",
    "                    T_U   = logistic(ell_U)\n",
    "                    \n",
    "                    # mis-specified joint (dependence neglect)\n",
    "                    ell_hat = ell_H + ell_A - ell_Y\n",
    "                    T_hat   = logistic(ell_hat)\n",
    "                    \n",
    "                    # accumulate losses\n",
    "                    loss_H     += prob * log_loss(y, T_H)\n",
    "                    loss_A     += prob * log_loss(y, T_A)\n",
    "                    loss_joint += prob * log_loss(y, T_U)\n",
    "                    loss_mis   += prob * log_loss(y, T_hat)\n",
    "    \n",
    "    # Information-theoretic derived quantities\n",
    "    v_A      = prior_loss - loss_A        # value of A alone\n",
    "    v_A_given_H = loss_H - loss_joint     # marginal value of A given H\n",
    "    overlap_alpha = (n_both / n_H) if n_H > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        \"H(Y)\": prior_loss,\n",
    "        \"H(Y|H)\": loss_H,\n",
    "        \"H(Y|A)\": loss_A,\n",
    "        \"H(Y|H,A)\": loss_joint,\n",
    "        \"E[loss_misspecified]\": loss_mis,\n",
    "        \"v(A)\": v_A,\n",
    "        \"v(A|H)\": v_A_given_H,\n",
    "        \"alpha_candidate\": overlap_alpha,\n",
    "        \"n_H\": n_H,\n",
    "        \"n_A\": n_A,\n",
    "        \"n_both\": n_both\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aedc7a",
   "metadata": {},
   "source": [
    "Quick test (non-interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5acf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H(Y)': np.float64(0.6931471805599453),\n",
       " 'H(Y|H)': np.float64(0.04513945272073848),\n",
       " 'H(Y|A)': np.float64(0.013050227549253505),\n",
       " 'H(Y|H,A)': np.float64(0.0038776516658284503),\n",
       " 'E[loss_misspecified]': np.float64(0.007553455329953645),\n",
       " 'v(A)': np.float64(0.6800969530106917),\n",
       " 'v(A|H)': np.float64(0.04126180105491003),\n",
       " 'alpha_candidate': 0.5,\n",
       " 'n_H': 10,\n",
       " 'n_A': 15,\n",
       " 'n_both': 5}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_test = compute_exact_losses(\n",
    "    n_H=10, n_A=15, n_both=5,\n",
    "    p_Y=0.5, p0=0.2, p1=0.8\n",
    ")\n",
    "res_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5fbf0",
   "metadata": {},
   "source": [
    "### Basic bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800779c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0c79bc0bd84885ab372dddd83a0350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='|H|', max=40), IntSlider(value=10, description='|A|', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses_exact(\n",
    "    n_H=10, n_A=10, n_both=5,\n",
    "    p_Y=0.5, p0=0.3, p1=0.7\n",
    "):\n",
    "    results = compute_exact_losses(n_H, n_A, n_both, p_Y, p0, p1)\n",
    "    \n",
    "    labels = [\"H(Y)\", \"H(Y|H)\", \"H(Y|A)\", \"H(Y|H,A)\", \"misspec\"]\n",
    "    values = [\n",
    "        results[\"H(Y)\"],\n",
    "        results[\"H(Y|H)\"],\n",
    "        results[\"H(Y|A)\"],\n",
    "        results[\"H(Y|H,A)\"],\n",
    "        results[\"E[loss_misspecified]\"],\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    bars = plt.bar(labels, values)\n",
    "    plt.ylabel(\"Expected log loss (nats)\")\n",
    "    plt.title(\n",
    "        f\"p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f}, \"\n",
    "        f\"|H|={n_H}, |A|={n_A}, |H∩A|={min(n_both, n_H, n_A)}\"\n",
    "    )\n",
    "    for b, v in zip(bars, values):\n",
    "        plt.text(b.get_x() + b.get_width()/2, v, f\"{v:.3f}\",\n",
    "                 ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    plt.ylim(0, max(values) * 1.15)\n",
    "    plt.show()\n",
    "    \n",
    "    # print(\"alpha candidate (Cov/Var) =\", results[\"alpha_candidate\"])\n",
    "    # print(\"v(A)      = H(Y) - H(Y|A)      =\", results[\"v(A)\"])\n",
    "    # print(\"v(A|H)    = H(Y|H) - H(Y|H,A)  =\", results[\"v(A|H)\"])\n",
    "    # print(\"H(Y|H,A) optimal joint        =\", results[\"H(Y|H,A)\"])\n",
    "    # print(\"Misspecified joint loss       =\", results[\"E[loss_misspecified]\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        plot_losses_exact,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=40, step=1, value=10),\n",
    "        n_A=IntSlider(description=\"|A|\", min=0, max=40, step=1, value=10),\n",
    "        n_both=IntSlider(description=\"|H∩A|\", min=0, max=40, step=1, value=5),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\",  min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\",  min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use sliders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c99851",
   "metadata": {},
   "source": [
    "### Loss vs. $|\\Aset|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf8369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e564508bfa9497b856208b450628143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='|H|', max=40), IntSlider(value=5, description='|H∩A|', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sweep_over_A(\n",
    "    n_H=10,\n",
    "    n_both_base=5,\n",
    "    p_Y=0.5,\n",
    "    p0=0.3,\n",
    "    p1=0.7,\n",
    "    max_A=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Fix n_H, nominal n_both (overlap), and p_Y, p0, p1.\n",
    "    Plot how losses change as |A| varies from 0 to max_A.\n",
    "    \n",
    "    Note: for each |A|, the *effective* overlap is min(n_both_base, n_H, |A|).\n",
    "    \"\"\"\n",
    "    A_values = list(range(0, max_A + 1))\n",
    "    loss_H_vals      = []\n",
    "    loss_A_vals      = []\n",
    "    loss_joint_vals  = []\n",
    "    loss_mis_vals    = []\n",
    "    \n",
    "    for n_A in A_values:\n",
    "        # effective overlap\n",
    "        n_both_eff = min(n_both_base, n_H, n_A)\n",
    "        res = compute_exact_losses(\n",
    "            n_H=n_H,\n",
    "            n_A=n_A,\n",
    "            n_both=n_both_eff,\n",
    "            p_Y=p_Y,\n",
    "            p0=p0,\n",
    "            p1=p1\n",
    "        )\n",
    "        loss_H_vals.append(res[\"H(Y|H)\"])\n",
    "        loss_A_vals.append(res[\"H(Y|A)\"])\n",
    "        loss_joint_vals.append(res[\"H(Y|H,A)\"])\n",
    "        loss_mis_vals.append(res[\"E[loss_misspecified]\"])\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(A_values, loss_H_vals,     label=\"H-only loss H(Y|H)\")\n",
    "    plt.plot(A_values, loss_A_vals,     label=\"A-only loss H(Y|A)\")\n",
    "    plt.plot(A_values, loss_joint_vals, label=\"Optimal joint H(Y|H,A)\")\n",
    "    plt.plot(A_values, loss_mis_vals,   label=\"Misspecified joint loss\")\n",
    "    \n",
    "    plt.xlabel(\"|A|\")\n",
    "    plt.ylabel(\"Expected log loss (nats)\")\n",
    "    plt.title(\n",
    "        f\"Loss vs |A| (|H|={n_H}, nominal |H∩A|={n_both_base}, \"\n",
    "        f\"p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f})\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget for sweep over |A|\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        sweep_over_A,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=40, step=1, value=10),\n",
    "        n_both_base=IntSlider(description=\"nom. |H∩A|\", min=0, max=40, step=1, value=5),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\",  min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\",  min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use the sliders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a9ea4",
   "metadata": {},
   "source": [
    "### Loss vs. $|\\HAunion|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "268e23c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17165d61c0eb44b796d54acdb8939f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='|H|', max=40), IntSlider(value=10, description='|A|', m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sweep_over_overlap(\n",
    "    n_H=10,\n",
    "    n_A=10,\n",
    "    p_Y=0.5,\n",
    "    p0=0.3,\n",
    "    p1=0.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Fix n_H, n_A, p_Y, p0, p1.\n",
    "    Plot how losses change as |H∩A| varies from 0 to min(|H|,|A|).\n",
    "    \"\"\"\n",
    "    max_both = min(n_H, n_A)\n",
    "    both_values = list(range(0, max_both + 1))\n",
    "    \n",
    "    loss_H_vals      = []\n",
    "    loss_A_vals      = []\n",
    "    loss_joint_vals  = []\n",
    "    loss_mis_vals    = []\n",
    "    \n",
    "    for n_both in both_values:\n",
    "        res = compute_exact_losses(\n",
    "            n_H=n_H,\n",
    "            n_A=n_A,\n",
    "            n_both=n_both,\n",
    "            p_Y=p_Y,\n",
    "            p0=p0,\n",
    "            p1=p1\n",
    "        )\n",
    "        loss_H_vals.append(res[\"H(Y|H)\"])\n",
    "        loss_A_vals.append(res[\"H(Y|A)\"])\n",
    "        loss_joint_vals.append(res[\"H(Y|H,A)\"])\n",
    "        loss_mis_vals.append(res[\"E[loss_misspecified]\"])\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(both_values, loss_H_vals,     label=\"H-only loss H(Y|H)\")\n",
    "    plt.plot(both_values, loss_A_vals,     label=\"A-only loss H(Y|A)\")\n",
    "    plt.plot(both_values, loss_joint_vals, label=\"Optimal joint H(Y|H,A)\")\n",
    "    plt.plot(both_values, loss_mis_vals,   label=\"Misspecified joint loss\")\n",
    "    \n",
    "    plt.xlabel(\"|H∩A|\")\n",
    "    plt.ylabel(\"Expected log loss (nats)\")\n",
    "    plt.title(\n",
    "        f\"Loss vs |H∩A| (|H|={n_H}, |A|={n_A}, \"\n",
    "        f\"p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f})\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget for sweep over |H∩A|\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        sweep_over_overlap,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=40, step=1, value=10),\n",
    "        n_A=IntSlider(description=\"|A|\", min=0, max=40, step=1, value=10),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\",  min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\",  min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use the sliders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a024fc",
   "metadata": {},
   "source": [
    "### 2D phase diagram: $|\\Aset|$ (y-axis) vs. $|\\HAunion|$ (x-axis)\n",
    "\n",
    "Y-axis $|\\Aset|$ is monotone with signal quality, so greater y-axis means better AI signal.\n",
    "\n",
    "Greater x-axis means more overlap.\n",
    "\n",
    "Since $|\\Hset|$ is fixed for this analysis, this is equivalent to plotting $\\alpha = \\Cov (\\ell_H, \\ell_A | Y) / \\Var (\\ell_H | Y)$ as the x-axis (just that the x-axis would then range from 0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8828a393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b322f5fe31a484cb399e6aef8c45423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='|H|', max=40), FloatSlider(value=0.5, description='p_Y'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "\n",
    "def classify_point(n_H, n_A, n_both, p_Y, p0, p1):\n",
    "    \"\"\"\n",
    "    For a given (n_H, n_A, n_both, p_Y, p0, p1), classify which regime we are in:\n",
    "      0 = complementarity (misspecified joint < min(H-only, A-only))\n",
    "      1 = human alone best\n",
    "      2 = AI alone best\n",
    "    Returns (category, losses_dict).\n",
    "    \"\"\"\n",
    "    res = compute_exact_losses(n_H=n_H, n_A=n_A, n_both=n_both,\n",
    "                               p_Y=p_Y, p0=p0, p1=p1)\n",
    "    loss_H   = res[\"H(Y|H)\"]\n",
    "    loss_A   = res[\"H(Y|A)\"]\n",
    "    loss_mis = res[\"E[loss_misspecified]\"]\n",
    "    \n",
    "    # Complementarity: misspecified joint strictly better than either alone\n",
    "    if loss_mis < min(loss_H, loss_A):\n",
    "        cat = 0\n",
    "    # Human alone best\n",
    "    elif loss_H <= loss_A and loss_H <= loss_mis:\n",
    "        cat = 1\n",
    "    # AI alone best\n",
    "    elif loss_A <= loss_H and loss_A <= loss_mis:\n",
    "        cat = 2\n",
    "    else:\n",
    "        # Fallback for strange tie cases\n",
    "        cat = 1\n",
    "    return cat, res\n",
    "\n",
    "\n",
    "def plot_region_HA_grid(\n",
    "    n_H=10,\n",
    "    p_Y=0.5,\n",
    "    p0=0.3,\n",
    "    p1=0.7,\n",
    "    max_A=40\n",
    "):\n",
    "    \"\"\"\n",
    "    X-axis: |H ∩ A|\n",
    "    Y-axis: |A|\n",
    "    Fixed parameters: |H|, p_Y, p0, p1.\n",
    "    \n",
    "    Each point (|H∩A|, |A|) is colored by:\n",
    "      - Complementarity (misspec joint < both H-only and A-only)\n",
    "      - Human alone best\n",
    "      - AI alone best\n",
    "    \"\"\"\n",
    "    # X: overlap size from 0..n_H\n",
    "    max_both = n_H\n",
    "    x_vals = list(range(0, max_both + 1))   # |H∩A|\n",
    "    # Y: A size from 0..max_A\n",
    "    y_vals = list(range(0, max_A + 1))      # |A|\n",
    "    \n",
    "    # data[y, x] = category in {0,1,2}, or -1 for invalid (overlap > |A| or > |H|)\n",
    "    data = -1 * np.ones((len(y_vals), len(x_vals)), dtype=int)\n",
    "    \n",
    "    for iy, n_A in enumerate(y_vals):\n",
    "        for ix, n_both in enumerate(x_vals):\n",
    "            # invalid states: overlap can't exceed |H| or |A|\n",
    "            if n_both > n_H or n_both > n_A:\n",
    "                continue\n",
    "            cat, _ = classify_point(n_H=n_H, n_A=n_A, n_both=n_both,\n",
    "                                    p_Y=p_Y, p0=p0, p1=p1)\n",
    "            data[iy, ix] = cat\n",
    "    \n",
    "    # Create a masked array so invalid points show up as blank\n",
    "    masked = np.ma.masked_where(data < 0, data)\n",
    "    \n",
    "    # colormap for 3 categories: 0=complementarity, 1=human, 2=AI\n",
    "    cmap = ListedColormap([\"tab:green\", \"tab:blue\", \"tab:orange\"])\n",
    "    \n",
    "    plt.figure(figsize=(7, 6))\n",
    "    im = plt.imshow(\n",
    "        masked,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        cmap=cmap,\n",
    "        extent=[x_vals[0] - 0.5, x_vals[-1] + 0.5, y_vals[0] - 0.5, y_vals[-1] + 0.5]\n",
    "    )\n",
    "    plt.colorbar(\n",
    "        im,\n",
    "        ticks=[0, 1, 2],\n",
    "        label=\"Regime\"\n",
    "    )\n",
    "    \n",
    "    # Relabel colorbar ticks\n",
    "    cbar = plt.gci().colorbar\n",
    "    cbar.ax.set_yticklabels([\n",
    "        \"Complementarity\\n(misspec joint best)\",\n",
    "        \"Human alone best\",\n",
    "        \"AI alone best\"\n",
    "    ])\n",
    "    \n",
    "    plt.xlabel(\"|H∩A|\")\n",
    "    plt.ylabel(\"|A|\")\n",
    "    plt.title(\n",
    "        f\"Best performer by (|H∩A|, |A|)\\n\"\n",
    "        f\"|H|={n_H}, p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f}\"\n",
    "    )\n",
    "    plt.grid(False)  # grid looks messy on imshow; leave off\n",
    "    \n",
    "    # Overlay valid region boundary (|H∩A| ≤ min(|H|, |A|))\n",
    "    # Draw the line |H∩A| = |A| and |H∩A| = |H| for visualization.\n",
    "    # Only the intersection of |H∩A|≤|H| and |H∩A|≤|A| is valid.\n",
    "    # We'll simply outline the triangle where n_both <= min(n_H, n_A) visually:\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for n_A in y_vals:\n",
    "        xs.append(min(n_A, n_H))\n",
    "        ys.append(n_A)\n",
    "    plt.plot(xs, ys, color=\"k\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        plot_region_HA_grid,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=40, step=1, value=20),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\",  min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\",  min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "        # max_A is fixed internally to 40, but you can expose it as a slider if you like.\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use the sliders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913e55a",
   "metadata": {},
   "source": [
    "### 2D phase diagram: $|\\Aset|$ (y-axis) vs. $\\frac{|\\HAunion|}{|\\Aset|}$ (x-axis)\n",
    "\n",
    "The x-axis uses the same resulting expression as the Gaussian _sampling_ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48dece86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94fc8cf16424b3cb9e08922a0120c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=20, description='|H|', max=40), FloatSlider(value=0.5, description='p_Y'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_region_overlap_ratio_scatter(\n",
    "    n_H=20,\n",
    "    p_Y=0.5,\n",
    "    p0=0.3,\n",
    "    p1=0.7,\n",
    "    max_A=40\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter version of the HA regime plot.\n",
    "\n",
    "    Underlying data:\n",
    "      - Fix |H|, p_Y, p0, p1.\n",
    "      - Enumerate over all integer pairs (|A|, |H∩A|) with\n",
    "          0 <= |A| <= max_A,\n",
    "          0 <= |H∩A| <= min(|H|, |A|).\n",
    "\n",
    "    Plot:\n",
    "      - X-axis: overlap ratio r = |H ∩ A| / |A|\n",
    "      - Y-axis: |A|\n",
    "      - Each valid (|A|, |H∩A|) shown as a scatter point.\n",
    "      - Color encodes regime:\n",
    "          0 = complementarity (misspec joint best)\n",
    "          1 = human alone best\n",
    "          2 = AI alone best\n",
    "\n",
    "    Note: the degenerate case |A|=0 is omitted, since r is undefined (0/0).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    cats = []\n",
    "\n",
    "    for n_A in range(0, max_A + 1):\n",
    "        # skip n_A = 0 because |H∩A| / |A| is undefined\n",
    "        if n_A == 0:\n",
    "            continue\n",
    "        for n_both in range(0, min(n_A, n_H) + 1):\n",
    "            cat, _ = classify_point(\n",
    "                n_H=n_H,\n",
    "                n_A=n_A,\n",
    "                n_both=n_both,\n",
    "                p_Y=p_Y,\n",
    "                p0=p0,\n",
    "                p1=p1,\n",
    "            )\n",
    "            ratio = n_both / n_A  # |H∩A| / |A|\n",
    "\n",
    "            xs.append(ratio)\n",
    "            ys.append(n_A)\n",
    "            cats.append(cat)\n",
    "\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "    cats = np.array(cats)\n",
    "\n",
    "    # Colors/labels for the three regimes\n",
    "    colors = {\n",
    "        0: \"tab:green\",   # complementarity\n",
    "        1: \"tab:blue\",    # human alone best\n",
    "        2: \"tab:orange\",  # AI alone best\n",
    "    }\n",
    "    labels = {\n",
    "        0: \"Complementarity\\n(misspec joint best)\",\n",
    "        1: \"Human alone best\",\n",
    "        2: \"AI alone best\",\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "\n",
    "    # Plot each category separately for legend labels\n",
    "    for cat in [0, 1, 2]:\n",
    "        mask = (cats == cat)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        plt.scatter(\n",
    "            xs[mask],\n",
    "            ys[mask],\n",
    "            s=30,\n",
    "            alpha=0.8,\n",
    "            color=colors[cat],\n",
    "            label=labels[cat],\n",
    "        )\n",
    "\n",
    "    plt.xlabel(r\"|H ∩ A| / |A|\")\n",
    "    plt.ylabel(r\"|A|\")\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(0, max_A + 1)\n",
    "    plt.title(\n",
    "        \"Best performer by overlap ratio and |A|\\n\"\n",
    "        f\"|H|={n_H}, p_Y={p_Y:.2f}, p0={p0:.2f}, p1={p1:.2f}\"\n",
    "    )\n",
    "    plt.grid(True, linestyle=\":\", alpha=0.4)\n",
    "    plt.legend(loc=\"best\", fontsize=9, frameon=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive widget\n",
    "if HAS_WIDGETS:\n",
    "    interact(\n",
    "        plot_region_overlap_ratio_scatter,\n",
    "        n_H=IntSlider(description=\"|H|\", min=0, max=40, step=1, value=20),\n",
    "        p_Y=FloatSlider(description=\"p_Y\", min=0.01, max=0.99, step=0.01, value=0.5),\n",
    "        p0=FloatSlider(description=\"p0\",  min=0.01, max=0.99, step=0.01, value=0.3),\n",
    "        p1=FloatSlider(description=\"p1\",  min=0.01, max=0.99, step=0.01, value=0.7),\n",
    "        # max_A is fixed internally to 40, but you can expose it as a slider if you like.\n",
    "    );\n",
    "else:\n",
    "    print(\"ipywidgets not available; install it to use the sliders.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
